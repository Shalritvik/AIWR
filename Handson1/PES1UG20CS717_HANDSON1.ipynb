{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UE20CS332 : Algorithms For Intelligence Web And Information Retrieval \n",
    "\n",
    "SRN : PES1UG20CS717\n",
    "\n",
    "Name : Shal Ritvik Sinha\n",
    "\n",
    "Section : L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import emot\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import num2words\n",
    "import inflect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Shalritvik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Shalritvik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Shalritvik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Shalritvik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stemmer = nltk.stem.SnowballStemmer('english')\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1956967341</td>\n",
       "      <td>empty</td>\n",
       "      <td>xoshayzers</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1956967666</td>\n",
       "      <td>sadness</td>\n",
       "      <td>wannamama</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1956967696</td>\n",
       "      <td>sadness</td>\n",
       "      <td>coolfunky</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1956967789</td>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>czareaquino</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1956968416</td>\n",
       "      <td>neutral</td>\n",
       "      <td>xkilljoyx</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tweet_id   sentiment       author  \\\n",
       "0  1956967341       empty   xoshayzers   \n",
       "1  1956967666     sadness    wannamama   \n",
       "2  1956967696     sadness    coolfunky   \n",
       "3  1956967789  enthusiasm  czareaquino   \n",
       "4  1956968416     neutral    xkilljoyx   \n",
       "\n",
       "                                             content  \n",
       "0  @tiffanylue i know  i was listenin to bad habi...  \n",
       "1  Layin n bed with a headache  ughhhh...waitin o...  \n",
       "2                Funeral ceremony...gloomy friday...  \n",
       "3               wants to hang out with friends SOON!  \n",
       "4  @dannycastillo We want to trade with someone w...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('text_emotion.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df[\"content\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[:1000]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A.Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_corpus(content):\n",
    "    return list(map(word_tokenize, content))\n",
    "tokenized_corpus = tokenize_corpus(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet 0: ['@', 'tiffanylue', 'i', 'know', 'i', 'was', 'listenin', 'to', 'bad', 'habit', 'earlier', 'and', 'i', 'started', 'freakin', 'at', 'his', 'part', '=', '[']\n",
      "Tweet 1: ['Layin', 'n', 'bed', 'with', 'a', 'headache', 'ughhhh', '...', 'waitin', 'on', 'your', 'call', '...']\n",
      "Tweet 2: ['Funeral', 'ceremony', '...', 'gloomy', 'friday', '...']\n",
      "Tweet 3: ['wants', 'to', 'hang', 'out', 'with', 'friends', 'SOON', '!']\n",
      "Tweet 4: ['@', 'dannycastillo', 'We', 'want', 'to', 'trade', 'with', 'someone', 'who', 'has', 'Houston', 'tickets', ',', 'but', 'no', 'one', 'will', '.']\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f\"Tweet {i}: {tokenized_corpus[i]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E.Stopword removal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet 0: ['@', 'tiffanylue', 'know', 'listenin', 'bad', 'habit', 'earlier', 'started', 'freakin', 'part', '=', '[']\n",
      "Tweet 1: ['Layin', 'n', 'bed', 'headache', 'ughhhh', '...', 'waitin', 'call', '...']\n",
      "Tweet 2: ['Funeral', 'ceremony', '...', 'gloomy', 'friday', '...']\n",
      "Tweet 3: ['wants', 'hang', 'friends', 'SOON', '!']\n",
      "Tweet 4: ['@', 'dannycastillo', 'We', 'want', 'trade', 'someone', 'Houston', 'tickets', ',', 'one', '.']\n"
     ]
    }
   ],
   "source": [
    "def remove_stopwords(tokenized_tweets):\n",
    "    _function = lambda tweet: [word for word in tweet if word not in stopwords]\n",
    "    return list(map(_function, tokenized_tweets))\n",
    "\n",
    "tokenized_nostopw_corpus = remove_stopwords(tokenized_corpus)\n",
    "for i in range(5):\n",
    "    print(f\"Tweet {i}: {tokenized_nostopw_corpus[i]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B.Case Folding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def case_folding(tokenized_tweets):\n",
    "    _function = lambda tweet: [word.lower() for word in tweet]\n",
    "    return list(map(_function, tokenized_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_nostopw_case_corpus = case_folding(tokenized_nostopw_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet 0: ['@', 'tiffanylue', 'know', 'listenin', 'bad', 'habit', 'earlier', 'started', 'freakin', 'part', '=', '[']\n",
      "Tweet 1: ['layin', 'n', 'bed', 'headache', 'ughhhh', '...', 'waitin', 'call', '...']\n",
      "Tweet 2: ['funeral', 'ceremony', '...', 'gloomy', 'friday', '...']\n",
      "Tweet 3: ['wants', 'hang', 'friends', 'soon', '!']\n",
      "Tweet 4: ['@', 'dannycastillo', 'we', 'want', 'trade', 'someone', 'houston', 'tickets', ',', 'one', '.']\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f\"Tweet {i}: {tokenized_nostopw_case_corpus[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1956967341</td>\n",
       "      <td>empty</td>\n",
       "      <td>xoshayzers</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1956967666</td>\n",
       "      <td>sadness</td>\n",
       "      <td>wannamama</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1956967696</td>\n",
       "      <td>sadness</td>\n",
       "      <td>coolfunky</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1956967789</td>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>czareaquino</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1956968416</td>\n",
       "      <td>neutral</td>\n",
       "      <td>xkilljoyx</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tweet_id   sentiment       author  \\\n",
       "0  1956967341       empty   xoshayzers   \n",
       "1  1956967666     sadness    wannamama   \n",
       "2  1956967696     sadness    coolfunky   \n",
       "3  1956967789  enthusiasm  czareaquino   \n",
       "4  1956968416     neutral    xkilljoyx   \n",
       "\n",
       "                                             content  \n",
       "0  @tiffanylue i know  i was listenin to bad habi...  \n",
       "1  Layin n bed with a headache  ughhhh...waitin o...  \n",
       "2                Funeral ceremony...gloomy friday...  \n",
       "3               wants to hang out with friends SOON!  \n",
       "4  @dannycastillo We want to trade with someone w...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Removal of punctuation marks, emoticons, HTML tags and links\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet 0: ['', 'tiffanylue', 'know', 'listenin', 'bad', 'habit', 'earlier', 'started', 'freakin', 'part', '', '']\n",
      "Tweet 1: ['layin', 'n', 'bed', 'headache', 'ughhhh', '', 'waitin', 'call', '']\n",
      "Tweet 2: ['funeral', 'ceremony', '', 'gloomy', 'friday', '']\n",
      "Tweet 3: ['wants', 'hang', 'friends', 'soon', '']\n",
      "Tweet 4: ['', 'dannycastillo', 'we', 'want', 'trade', 'someone', 'houston', 'tickets', '', 'one', '']\n"
     ]
    }
   ],
   "source": [
    "def remove_punc(tokenized_tweets):\n",
    "    _function = lambda tweet: [re.sub(r'[^\\w\\s]', '', word) for word in tweet]\n",
    "    return list(map(_function, tokenized_tweets))\n",
    "removed_punc = remove_punc(tokenized_nostopw_case_corpus)\n",
    "for i in range(5):\n",
    "    print(f\"Tweet {i}: {removed_punc[i]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet 0: ['', 'tiffanylue', 'know', 'listenin', 'bad', 'habit', 'earlier', 'started', 'freakin', 'part', '', '']\n",
      "Tweet 1: ['layin', 'n', 'bed', 'headache', 'ughhhh', '', 'waitin', 'call', '']\n",
      "Tweet 2: ['funeral', 'ceremony', '', 'gloomy', 'friday', '']\n",
      "Tweet 3: ['wants', 'hang', 'friends', 'soon', '']\n",
      "Tweet 4: ['', 'dannycastillo', 'we', 'want', 'trade', 'someone', 'houston', 'tickets', '', 'one', '']\n"
     ]
    }
   ],
   "source": [
    "def remove_links(tokenized_tweets):\n",
    "    _function = lambda tweet: [re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ',word) for word in tweet]\n",
    "    return list(map(_function, tokenized_tweets))\n",
    "removed_links= remove_links(removed_punc)\n",
    "for i in range(5):\n",
    "    print(f\"Tweet {i}: {removed_links[i]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remving HTML tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet 0: ['', 'tiffanylue', 'know', 'listenin', 'bad', 'habit', 'earlier', 'started', 'freakin', 'part', '', '']\n",
      "Tweet 1: ['layin', 'n', 'bed', 'headache', 'ughhhh', '', 'waitin', 'call', '']\n",
      "Tweet 2: ['funeral', 'ceremony', '', 'gloomy', 'friday', '']\n",
      "Tweet 3: ['wants', 'hang', 'friends', 'soon', '']\n",
      "Tweet 4: ['', 'dannycastillo', 'we', 'want', 'trade', 'someone', 'houston', 'tickets', '', 'one', '']\n"
     ]
    }
   ],
   "source": [
    "def remove_html(tokenized_tweets):\n",
    "    _function = lambda tweet: [re.sub('<.*?>', '', word) for word in tweet]\n",
    "    return list(map(_function, tokenized_tweets))\n",
    "\n",
    "removed_html = remove_html(removed_punc)\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"Tweet {i}: {removed_html[i]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet 0: ['', 'tiffanylue', 'know', 'listenin', 'bad', 'habit', 'earlier', 'started', 'freakin', 'part', '', '']\n",
      "Tweet 1: ['layin', 'n', 'bed', 'headache', 'ughhhh', '', 'waitin', 'call', '']\n",
      "Tweet 2: ['funeral', 'ceremony', '', 'gloomy', 'friday', '']\n",
      "Tweet 3: ['wants', 'hang', 'friends', 'soon', '']\n",
      "Tweet 4: ['', 'dannycastillo', 'we', 'want', 'trade', 'someone', 'houston', 'tickets', '', 'one', '']\n"
     ]
    }
   ],
   "source": [
    "def remove_emoji(tokenized_tweets):\n",
    "    emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "    _function = lambda tweet: [re.sub(emoj, '', word) for word in tweet]\n",
    "    return list(map(_function, tokenized_tweets))\n",
    "removed_emoji = remove_emoji(removed_html)\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"Tweet {i}: {removed_emoji[i]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D. Convert numerical values to text (Ex: 10 -> “ten”)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet 0: ['', 'tiffanylue', 'know', 'listenin', 'bad', 'habit', 'earlier', 'started', 'freakin', 'part', '', '']\n",
      "Tweet 1: ['layin', 'n', 'bed', 'headache', 'ughhhh', '', 'waitin', 'call', '']\n",
      "Tweet 2: ['funeral', 'ceremony', '', 'gloomy', 'friday', '']\n",
      "Tweet 3: ['wants', 'hang', 'friends', 'soon', '']\n",
      "Tweet 4: ['', 'dannycastillo', 'we', 'want', 'trade', 'someone', 'houston', 'tickets', '', 'one', '']\n"
     ]
    }
   ],
   "source": [
    "def convert_to_words(tokenized_tweets):  \n",
    "    p = inflect.engine()\n",
    "    _function = lambda tweet: [inflect.engine().number_to_words(int(word)) if word.isdigit() else word for word in tweet]\n",
    "    return list(map(_function, tokenized_tweets)) \n",
    "converted_to_words = convert_to_words(removed_emoji)\n",
    "for i in range(5):\n",
    "    print(f\"Tweet {i}: {converted_to_words[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "G.Lemmatizing Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_words(tokenized_tweets):\n",
    "    _function = lambda tweet: [lemmatizer.lemmatize(word) for word in tweet]\n",
    "    return list(map(_function, tokenized_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_nostopw_case_lemm_corpus = lemmatize_words(converted_to_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet 0: ['', 'tiffanylue', 'know', 'listenin', 'bad', 'habit', 'earlier', 'started', 'freakin', 'part', '', '']\n",
      "Tweet 1: ['layin', 'n', 'bed', 'headache', 'ughhhh', '', 'waitin', 'call', '']\n",
      "Tweet 2: ['funeral', 'ceremony', '', 'gloomy', 'friday', '']\n",
      "Tweet 3: ['want', 'hang', 'friend', 'soon', '']\n",
      "Tweet 4: ['', 'dannycastillo', 'we', 'want', 'trade', 'someone', 'houston', 'ticket', '', 'one', '']\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f\"Tweet {i}: {tokenized_nostopw_case_lemm_corpus[i]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_words(tokenized_tweets):\n",
    "    _function = lambda tweet: [stemmer.stem(word) for word in tweet]\n",
    "    return list(map(_function, tokenized_tweets))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_nostopw_case_stem_corpus = lemmatize_words(tokenized_nostopw_case_lemm_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet 0: ['', 'tiffanylue', 'know', 'listenin', 'bad', 'habit', 'earlier', 'started', 'freakin', 'part', '', '']\n",
      "Tweet 1: ['layin', 'n', 'bed', 'headache', 'ughhhh', '', 'waitin', 'call', '']\n",
      "Tweet 2: ['funeral', 'ceremony', '', 'gloomy', 'friday', '']\n",
      "Tweet 3: ['want', 'hang', 'friend', 'soon', '']\n",
      "Tweet 4: ['', 'dannycastillo', 'we', 'want', 'trade', 'someone', 'houston', 'ticket', '', 'one', '']\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f\"Tweet {i}: {tokenized_nostopw_case_stem_corpus[i]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d2ed333285d07806df798bff5b25a6bcc1964efded36751b4c49dce21477a7ea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
